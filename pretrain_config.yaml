# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
device_target: "GPU"
# ==============================================================================
description: 'run_pretrain'
distribute: False
epoch: 4
device_id: 0
device_num: 10
train_batch_size: 2
eval_batch_size: 16
enable_save_ckpt: True
enable_lossscale: True
do_shuffle: True
enable_data_sink: False
data_sink_steps: 20
accumulation_steps: 1
save_checkpoint_path: './model/en_test'
load_checkpoint_path: ''
save_checkpoint_steps: 5000
train_steps: 400
save_checkpoint_num: 1
data_dir: '/home/huwenyan/waxProject/mindspore/albert/data/pre_train/train_data/run'
schema_dir: ''

# ==============================================================================
# pretrain related
albert_network: 'albert_base'
loss_scale_value: 65536
scale_factor: 2
scale_window: 1000
enable_global_norm: False
# pretrain_eval related
data_file: "/home/huwenyan/waxProject/mindspore/albert/data/pre_train/train_data/run"
schema_file: ""
finetune_ckpt: "/home/huwenyan/waxProject/mindspore/albert/moveckpt/albert_base.ckpt"
# optimizer related
optimizer_cfg:
    optimizer: 'AdamWeightDecay'
    AdamWeightDecay:
        learning_rate: 0.00003  # 3e-5
        end_learning_rate: 0.0
        power: 5.0
        weight_decay: 0.00001  # 1e-5
        decay_filter: ['layernorm', 'bias']
        eps: 0.000001  # 1e-6
        warmup_steps: 10000

    Lamb:
        learning_rate: 0.00176  # 3e-4
        end_learning_rate: 0.0
        power: 1.0
        warmup_steps: 10000
        weight_decay: 0.01
        decay_filter: ['layernorm', 'bias']
        eps: 0.000001  # 1e-8,



# ==============================================================================
albert_base:
    seq_length: 128
    vocab_size: 30000
    hidden_size: 768
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    inner_group_num: 1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

albert_large:
    seq_length: 128
    vocab_size: 30000
    hidden_size: 1024
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 24
    num_attention_heads: 16
    intermediate_size: 4096
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    inner_group_num: 1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

albert_xlarge:
    seq_length: 128
    vocab_size: 30000
    hidden_size: 2048
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 24
    num_attention_heads: 32
    intermediate_size: 8192
    hidden_act: "gelu"
    hidden_dropout_prob: 0
    inner_group_num: 1
    attention_probs_dropout_prob: 0
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

albert_xxlarge:
    seq_length: 128
    vocab_size: 30000
    hidden_size: 4096
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 12
    num_attention_heads: 64
    intermediate_size: 16384
    hidden_act: "gelu"
    hidden_dropout_prob: 0
    inner_group_num: 1
    attention_probs_dropout_prob: 0
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16


---
# Help description for each configuration
enable_modelarts: "Whether training on modelarts, default: False"
data_path: "The location of the input data."
output_path: "The location of the output file."
device_target: "Running platform, choose from Ascend or CPU, and default is Ascend."
enable_profiling: 'Whether enable profiling while training, default: False'

distribute: "Run distribute, default is 'false'."
epoch_size: "Epoch size, default is 1."
enable_save_ckpt: "Enable save checkpoint, default is true."
enable_lossscale: "Use lossscale or not, default is not."
do_shuffle: "Enable shuffle for dataset, default is true."
enable_data_sink: "Enable data sink, default is true."
data_sink_steps: "Sink steps for each epoch, default is 1."
accumulation_steps: "Accumulating gradients N times before weight update, default is 1."
allreduce_post_accumulation: "Whether to allreduce after accumulation of N steps or after each step, default is true."
save_checkpoint_path: "Save checkpoint path"
load_checkpoint_path: "Load checkpoint file path"
save_checkpoint_steps: "Save checkpoint steps, default is 1000"
train_steps: "Training Steps, default is -1, meaning run all steps according to epoch number."
save_checkpoint_num: "Save checkpoint numbers, default is 1."
data_dir: "Data path, it is better to use absolute path"
schema_dir: "Schema path, it is better to use absolute path"
---
# chocies
device_target: ['Ascend', 'GPU']
do_train: [True, False]
do_eval: [True, False]
train_data_shuffle: [True, False]
eval_data_shuffle: [True, False]
optimizer: ['AdamWeightDecay','Lamb']
albert_network: ["albert_base","albert_large","albert_xlarge","albert_xxlarge"]

