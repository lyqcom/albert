# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
output_path: "/home/huwenyan/waxProject/mindspore/albert/output/squad_finetune"
device_target: "GPU"

# ==============================================================================
description: "run_squad_v1"
do_train: False
do_eval: True
distribute: True
epoch_num: 3
train_batch_size: 2
eval_batch_size: 6
n_best_size: 20
doc_stride: 128
max_query_length: 64
max_answer_length: 30
num_class: 2
do_lower_case: True
save_checkpoint_steps: 1000
train_data_shuffle: True
eval_data_shuffle: False
vocab_file_path: "/home/huwenyan/waxProject/mindspore/albert/moveckpt/30k-clean.vocab"
spm_model_file: "/home/huwenyan/waxProject/mindspore/albert/moveckpt/30k-clean.model"
eval_json_path: "/home/huwenyan/waxProject/mindspore/albert/data/finetune/SQuAD1.1/dev-v1.1.json"
train_json_path: "/home/huwenyan/waxProject/mindspore/albert/data/finetune/SQuAD1.1/train-v1.1.json"
train_mindrecord_file: "/home/huwenyan/waxProject/mindspore/albert/data/finetune/mindrecord/squad_v1/train.mindrecord"
eval_mindrecord_file: "/home/huwenyan/waxProject/mindspore/albert/data/finetune/mindrecord/squad_v1/eval.mindrecord"
eval_pkl_file: "/home/huwenyan/waxProject/mindspore/albert/data/finetune/mindrecord/squad_v1/eval.pkl"
save_finetune_checkpoint_path: "/home/huwenyan/waxProject/mindspore/albert/output/squad_finetune/"
#load_last_finetune_checkpoint_path: "./output/squad_finetune/distribute_ckpt/0/squad_1-2_2774_best.ckpt"
load_last_finetune_checkpoint_path: ""
load_pretrain_checkpoint_path: "/home/huwenyan/waxProject/mindspore/albert/moveckpt/albert_base.ckpt"
load_finetune_checkpoint_path: "/home/huwenyan/waxProject/mindspore/albert/output/squad_finetune/"
schema_file_path: ""
albert_network: 'albert_base'

#export
export_ckpt_file: "/data/users/user2/projects/waxProject/albert/output/squad_finetune/single_ckpt/squad_1-3_5549_best.ckpt"
export_file_name: "./output/export/squadv1"
file_format: "MINDIR"


optimizer_cfg:
    optimizer: 'AdamWeightDecay'
    AdamWeightDecay:
        learning_rate: 0.00005  # 5e-5
        end_learning_rate: 0.00000000000  # 1e-11
        power: 1.0
        weight_decay: 0.01  # 1e-3
        decay_filter: ['layernorm', 'bias']
        eps: 0.000001  # 1e-6
    Lamb:
        learning_rate: 0.0001  # 1e-4,
        end_learning_rate: 0.00000000001  # 1e-11
        power: 5.0
        weight_decay: 0.01
        decay_filter: ['layernorm', 'bias']

albert_base:
    seq_length: 384
    vocab_size: 30000
    hidden_size: 768
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 12
    num_attention_heads: 12
    intermediate_size: 3072
    hidden_act: "gelu"
    hidden_dropout_prob: 0.0
    inner_group_num: 1
    attention_probs_dropout_prob: 0.0
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

albert_large:
    seq_length: 384
    vocab_size: 30000
    hidden_size: 1024
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 24
    num_attention_heads: 16
    intermediate_size: 4096
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    inner_group_num: 1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

albert_xlarge:
    seq_length: 384
    vocab_size: 30000
    hidden_size: 2048
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 24
    num_attention_heads: 32
    intermediate_size: 8192
    hidden_act: "gelu"
    hidden_dropout_prob: 0
    inner_group_num: 1
    attention_probs_dropout_prob: 0
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

albert_xxlarge:
    seq_length: 384
    vocab_size: 30000
    hidden_size: 4096
    embedding_size: 128
    num_hidden_groups: 1
    num_hidden_layers: 12
    num_attention_heads: 64
    intermediate_size: 16384
    hidden_act: "gelu"
    hidden_dropout_prob: 0
    inner_group_num: 1
    attention_probs_dropout_prob: 0
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: True
    dtype: mstype.float32
    compute_type: mstype.float16

---
# Help description for each configuration
output_path: "The location of the output file."
device_target: "Running platform, choose from Ascend or CPU, and default is Ascend."

do_train: "Eable train, default is false"
do_eval: "Eable eval, default is false"
device_id: "Device id, default is 0."
epoch_num: "Epoch number, default is 1."
num_class: "The number of class, default is 2."
train_data_shuffle: "Enable train data shuffle, default is true"
eval_data_shuffle: "Enable eval data shuffle, default is false"
train_batch_size: "Train batch size, default is 32"
eval_batch_size: "Eval batch size, default is 1"
vocab_file_path: "Vocab file path"
train_json_path: "Evaluation json file path, can be train.json"
eval_json_path: "Evaluation json file path, can be eval.json"
spm_model_file: "A clean file of good words adapted to the model"
save_finetune_checkpoint_path: "Save checkpoint path"
load_pretrain_checkpoint_path: "Load checkpoint file path"
load_finetune_checkpoint_path: "Load checkpoint file path"
train_data_file_path: "Data path, it is better to use absolute path"
schema_file_path: "Schema path, it is better to use absolute path"

export_batch_size: "export batch size."
export_ckpt_file: "Bert ckpt file."
export_file_name: "bert output air name."
file_format: "file format"
---
# chocies
device_target: ['Ascend', 'GPU']
do_train: [True, False]
do_eval: [True, False]
train_data_shuffle: [True, False]
eval_data_shuffle: [True, False]
optimizer: ['AdamWeightDecay','Lamb']
albert_network: ["albert_base","albert_large","albert_xlarge","albert_xxlarge"]
